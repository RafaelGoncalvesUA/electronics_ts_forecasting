import multiprocessing
import os
from itertools import product
from time import perf_counter

# remove files generated by previous runs
# if os.path.exists("models"):
#     os.system("rm -rf models")

# os.system("mkdir models")

# if os.path.exists("out.csv"):
#     os.remove("out.csv")

# if os.path.exists("ignore.csv"):
#     os.remove("ignore.csv")


header = "var,input,model,args,lookback,forecast,mse,train_time,inf_time\n"
with open("out.csv", "w") as f:
    f.write(header)

NUM_BUILDINGS = 6


def custom_error_handler(e):
    print(f"Error: {e}")
    exit(1)


def run_job(data_path, results_path, model, lb, fc, red, is_global):
    print(f"Running {model} with lookback {lb}, forecast {fc}, and reduction {red}")
    
    red_method = f"-r {red}" if red else ""
    model_args = f"-a {model[1]}" if len(model) > 1 else ""
    is_global_flag = "-g " if is_global else ""
    
    command = f"python3 run.py -d data/preprocessed/{data_path[0]} -t data/preprocessed/{data_path[1]} {is_global_flag}-f {data_path[2]} "
    command += f"-o {results_path} -m {model[0]} -l {lb} -w {fc} {model_args} {red_method}"
    
    print(command)
    return os.system(command)

total_start = perf_counter()

MIN_COUNTER = -1

for mode in [("global", 0)]: #, ("global", -1), ("global", 1), ("local", 0)]:
    print(f"Training {mode[0].capitalize()} model with mode {mode[1]}")

    start = perf_counter()

    if mode == ("global", 0):
        datasets = [("", "b1_test.csv", 0)] # train global model from scratch (only once)
        out_file = "ignore.csv" # do not test
        is_global = True

    elif mode == ("local", 0):
        datasets = [(f"b{b}_local.csv", f"b{b}_test.csv", 0) for b in range(1, NUM_BUILDINGS+1)] # train local model from scratch
        out_file = "out.csv"
        is_global = False
    
    elif mode[1] == -1:
        datasets = [("", f"b{b}_test.csv", -1) for b in range(1, NUM_BUILDINGS+1)] # test global model for all buildings
        out_file = "out.csv"
        is_global = True

    elif mode[1] == 1:
        datasets = [(f"b{b}_local.csv", f"b{b}_test.csv", 1) for b in range(1, NUM_BUILDINGS+1)] # fine tune global model for all buildings
        out_file = "out.csv"
        is_global = False # because we are fine tuning the global model for each building

    models =  [("ann",)]

    lstm_units = [64, 128]
    dropout = [0.2]
    recurrent_dropout = [0.2]
    lstm_args = list(product(lstm_units, dropout, recurrent_dropout))
    models += [("lstm", f"{conf[0]},{conf[1]},{conf[2]}") for conf in lstm_args]

    cnn_units = [64]
    models += [("cnn", conf) for conf in cnn_units]

    lookback = [24, 48, 96]
    forecast = [1, 3, 5]
    reduction = ["vae", "pca", None]

    jobs = list(product(datasets, models, lookback, forecast, reduction))
    
    ctr = 0
    for data_path, model, lb, fc, red in jobs:
        if ctr % 20 == 0:
            with open("logs.txt", "a") as f:
                f.write(f"{mode[0]}_{mode[1]}: {ctr} / {len(jobs)} jobs\n")

        if ctr < MIN_COUNTER:
            ctr += 1
            continue

        if run_job(data_path, out_file, model, lb, fc, red, is_global):
            print(f"Error running job {ctr}")
            exit(1)

        ctr += 1

    # write to logs.txt
    with open("logs.txt", "a") as f:
        f.write(f"{mode[0]}_{mode[1]}: {ctr} jobs completed\n")

    end = perf_counter()
    print(f"Elapsed time: {end - start:.2f} seconds")

total_end = perf_counter()
print(f"Total elapsed time: {total_end - total_start:.2f} seconds")
